# Foundations of Diffusion Model

- Three predominant formulations: 三种主要公式 denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and stochastic differential equations (Score SDEs)
- Key: progressively destruct data with intensifying random noise (called the “diffusion” process), then successively remove noise to generate new data samples

# 2.1 Denoising Diffusion Probabilistic Models (DDPM)

- two Markov chains: a forward chain that perturbs data to noise, and a reverse chain that converts noise back to data.
    
    - forward chain: transform any data distribution into a simple prior distribution (e.g., standard Gaussian)
    - Second chain: reverses the former by learning transition kernels parameterized by deep neural networks
    - New data points are subsequently generated by first sampling a random vector from the prior distribution, followed by ancestral sampling through the reverse Markov chain
- Forward Chain Mathematical Form: Formally, given a data distribution $x_0 ∼ 𝑞(x_0)$, the forward Markov process generates a sequence of random variables $x_1,x_2...x_𝑇$ with transition kernel $𝑞(x_𝑡 |x_{𝑡−1})$. By chain rule of probability, we can write this sequence as a joint distribution:
    
    $$ q(x_1,x_2,\cdots,x_T|x_0)=\prod_{t=1}^{n}q(x_t|x_0) $$
    
    - Typical Transition Kernel:
      $$\begin{aligned} 
        &𝑞(x_𝑡 |x_{𝑡−1})=N(x_𝑡;\sqrt{1−𝛽_𝑡}x_𝑡−1,𝛽_𝑡I), \\ 
        &\beta_𝑡 ∈(0,1)\text{ is a hyperparameter chosen ahead of model training}
        \end{aligned} 
        $$
        
        Using this kernel, we have, by Sohl-Dickstein, the marginal distribution below:

        $$
        
        \begin{aligned} &𝑞(x_𝑡 |x_0)=N(x_𝑡; \sqrt{\overline{a}_t}x_0,(1−\overline{a}_𝑡)I)\\ &\text{where } a_t=1-\beta_t\text{, and } \overline{a}_t=\prod_{i=0}^ta_i \end{aligned} $$
        
        For large $T$, we have $\overline{a}_T\approx0$, thus, $q(x_T)\approx N(x_T;0,I)$
        
- Reverse Markov chain: parameterized by a prior distribution $𝑝 (x_𝑇 ) = N (x_𝑇 ; 0, I)$ and a learnable transition kernel $𝑝_𝜃 (x_{𝑡 −1}| x_𝑡 ).$ 
- Learnable transition kernel:
    
    $$ \begin{aligned}
    &𝑝_\theta(x_{t-1} |x_t)=N(x_{t-1} ;𝜇 (x_t,𝑡),Σ (x_t,𝑡)) \\
    &\text{𝜃 denotes model parameters,}\\
    &\text{and the mean 𝜇 (x ,𝑡) and variance Σ (x ,𝑡) are parameterized}\\ 
    &\text{by deep neural networks}
    \end{aligned} 
    $$
    
- Try to match the reverse markov chain to actual reversal of the forward Markov chain by minimizing the Kullback-Leibler (KL) divergence between these two chains:
	- $$ \begin{aligned} &KL(q(x_0,x_1,\cdots,x_T)||p_\theta(x_0,x_1,\cdots,x_T))\\ =&-\mathbb{E}_{q(x_0,x_1,\cdots,x_T)}[\log{p_\theta(x_0,x_1,\cdots,x_T)}]+\text{const}\quad\text{Definition of KL divergence}\\ =&-\mathbb{E}_{q(x_0,x_1,\cdots,x_T)}\bigg[-\log p(x_T)-\sum_{t=1}^T\log\frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\bigg]+\text{const}\quad\text{Definition of KL divergence}\\ \geq& \mathbb{E}[-\log p_\theta(x_0)]+const \end{aligned} $$
    
    - $\mathbb{E}_{q(x_0,x_1,\cdots,x_T)}\bigg[-\log p(x_T)-\sum_{t=1}^T\log\frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\bigg]$ is the **************************************************Variational Lower Bound (VLB),************************************************** the objective of DDPM is to maximize the MLB or minimizing the negative VLB

# 2.2 Score-Based Generative Models

# 2.3 Score Stochastic Differential Equations

Generalization of DDPMs and SGMs with infinite time steps. Perturb data to noise with a diffusion process governed by: $dx=f(x,t)dt+g(t)dw$ where f(x,t) and g(t) are diffusion and drift functions of the SDE, w is Brownian Motion.

- SDE for DDPM

$$ \begin{aligned} &dx=-\frac{1}{2}\beta(t)xdt+\sqrt{\beta(t)}dW(t)\\ &\text{where }\beta(\frac{t}{T})=T\beta_t \end{aligned} $$

- SDE for SGM: