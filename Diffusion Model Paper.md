# Foundations of Diffusion Model

- Three predominant formulations: ä¸‰ç§ä¸»è¦å…¬å¼ denoising diffusion probabilistic models (DDPMs), score-based generative models (SGMs), and stochastic differential equations (Score SDEs)
- Key: progressively destruct data with intensifying random noise (called the â€œdiffusionâ€ process), then successively remove noise to generate new data samples

# 2.1 Denoising Diffusion Probabilistic Models (DDPM)

- two Markov chains: a forward chain that perturbs data to noise, and a reverse chain that converts noise back to data.
    
    - forward chain: transform any data distribution into a simple prior distribution (e.g., standard Gaussian)
    - Second chain: reverses the former by learning transition kernels parameterized by deep neural networks
    - New data points are subsequently generated by first sampling a random vector from the prior distribution, followed by ancestral sampling through the reverse Markov chain
- Forward Chain Mathematical Form: Formally, given a data distribution $x_0 âˆ¼ ğ‘(x_0)$, the forward Markov process generates a sequence of random variables $x_1,x_2...x_ğ‘‡$ with transition kernel $ğ‘(x_ğ‘¡ |x_{ğ‘¡âˆ’1})$. By chain rule of probability, we can write this sequence as a joint distribution:
    
    $$ q(x_1,x_2,\cdots,x_T|x_0)=\prod_{t=1}^{n}q(x_t|x_0) $$
    
    - Typical Transition Kernel:
      $$\begin{aligned} 
        &ğ‘(x_ğ‘¡ |x_{ğ‘¡âˆ’1})=N(x_ğ‘¡;\sqrt{1âˆ’ğ›½_ğ‘¡}x_ğ‘¡âˆ’1,ğ›½_ğ‘¡I), \\ 
        &\beta_ğ‘¡ âˆˆ(0,1)\text{ is a hyperparameter chosen ahead of model training}
        \end{aligned} 
        $$
        
        Using this kernel, we have, by Sohl-Dickstein, the marginal distribution below:

        $$
        
        \begin{aligned} &ğ‘(x_ğ‘¡ |x_0)=N(x_ğ‘¡; \sqrt{\overline{a}_t}x_0,(1âˆ’\overline{a}_ğ‘¡)I)\\ &\text{where } a_t=1-\beta_t\text{, and } \overline{a}_t=\prod_{i=0}^ta_i \end{aligned} $$
        
        For large $T$, we have $\overline{a}_T\approx0$, thus, $q(x_T)\approx N(x_T;0,I)$
        
- Reverse Markov chain: parameterized by a prior distribution $ğ‘ (x_ğ‘‡ ) = N (x_ğ‘‡ ; 0, I)$ and a learnable transition kernel $ğ‘_ğœƒ (x_{ğ‘¡ âˆ’1}| x_ğ‘¡ ).$ 
- Learnable transition kernel:
    
    $$ \begin{aligned}
    &ğ‘_\theta(x_{t-1} |x_t)=N(x_{t-1} ;ğœ‡ (x_t,ğ‘¡),Î£ (x_t,ğ‘¡)) \\
    &\text{ğœƒ denotes model parameters,}\\
    &\text{and the mean ğœ‡ (x ,ğ‘¡) and variance Î£ (x ,ğ‘¡) are parameterized}\\ 
    &\text{by deep neural networks}
    \end{aligned} 
    $$
    
- Try to match the reverse markov chain to actual reversal of the forward Markov chain by minimizing the Kullback-Leibler (KL) divergence between these two chains:
	- $$ \begin{aligned} &KL(q(x_0,x_1,\cdots,x_T)||p_\theta(x_0,x_1,\cdots,x_T))\\ =&-\mathbb{E}_{q(x_0,x_1,\cdots,x_T)}[\log{p_\theta(x_0,x_1,\cdots,x_T)}]+\text{const}\quad\text{Definition of KL divergence}\\ =&-\mathbb{E}_{q(x_0,x_1,\cdots,x_T)}\bigg[-\log p(x_T)-\sum_{t=1}^T\log\frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\bigg]+\text{const}\quad\text{Definition of KL divergence}\\ \geq& \mathbb{E}[-\log p_\theta(x_0)]+const \end{aligned} $$
    
    - $\mathbb{E}_{q(x_0,x_1,\cdots,x_T)}\bigg[-\log p(x_T)-\sum_{t=1}^T\log\frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})}\bigg]$ is the **************************************************Variational Lower Bound (VLB),************************************************** the objective of DDPM is to maximize the MLB or minimizing the negative VLB

# 2.2 Score-Based Generative Models

# 2.3 Score Stochastic Differential Equations

Generalization of DDPMs and SGMs with infinite time steps. Perturb data to noise with a diffusion process governed by: $dx=f(x,t)dt+g(t)dw$ where f(x,t) and g(t) are diffusion and drift functions of the SDE, w is Brownian Motion.

- SDE for DDPM

$$ \begin{aligned} &dx=-\frac{1}{2}\beta(t)xdt+\sqrt{\beta(t)}dW(t)\\ &\text{where }\beta(\frac{t}{T})=T\beta_t \end{aligned} $$

- SDE for SGM: